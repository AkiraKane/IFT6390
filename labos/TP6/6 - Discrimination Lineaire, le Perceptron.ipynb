{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Classification lin\u00e9aire\n",
      "\n",
      "## Le Perceptron\n",
      "\n",
      "Durant ce travail pratique, vous allez impl\u00e9menter un algorithme de classification lin\u00e9aire bien connu: le perceptron. L'\u00e9quation de base du perceptron est la suivante:\n",
      "\n",
      "$$f(x) = h(\\pmb{w}^T x + b)$$ \n",
      "\n",
      "o\u00f9 \n",
      "\n",
      "$$ \n",
      " h(a) = \\left\\{ \n",
      "  \\begin{array}{l l}\n",
      "    ~1 & \\quad \\text{si} ~ a > 0\\\\\n",
      "    -1 & \\quad \\text{sinon}\n",
      "  \\end{array} \\right. $$\n",
      "\n",
      "et $x$ est un exemple \u00e0 pr\u00e9dire, $\\pmb{w}$ est un vecteur de poids et $b$ est un biais. $\\pmb{w}$ et $b$ sont les param\u00e8tres que nous allons devoir apprendre.\n",
      "\n",
      "## Apprentissage des param\u00e8tres pour le perceptron\n",
      "\n",
      "*Dans cette partie vous allez calculer les formules de mise \u00e0 jour des param\u00e8tres $\\pmb{w}$ et $b$*\n",
      "\n",
      "### Fonction objectif\n",
      "\n",
      "L'algorithme d'apprentissage du perceptron correspond \u00e0 une descente de gradient sur l\u2019objectif suivant:\n",
      "$$J_{perceptron}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n I_{\\{(\\pmb{w}^T x^{(i)} + b)t^{(i)} < 0\\}}(-(\\pmb{w}^T x^{(i)} + b)t^{(i)})$$\n",
      "\n",
      "En fait cet objectif va juste compter le nombre d'exemple qui sont mal classifi\u00e9s pour des param\u00e8tres $\\pmb{w}$ et $b$ donn\u00e9s. Si on r\u00e9duit la valeur de la fonction objectif, alors on r\u00e9duit le nombre d'exemple mal classifi\u00e9s, donc on obtient un meilleur classifieur.\n",
      "\n",
      "### Descente de gradient stochastique\n",
      "\n",
      "Pour obtenir l'algorithme d'apprentissage pour le perceptron, on va utiliser la m\u00e9thode de descente de gradient stochastique, c'est \u00e0 dire qu'on va mettre \u00e0 jour les poids de mani\u00e8re \u00e0 ce qu'\u00e0 chaque it\u00e9ration, on r\u00e9duise l'objectif calcul\u00e9 sur **un** exemple, en suivant la direction donn\u00e9e par l'oppos\u00e9 du gradient.\n",
      "\n",
      "\u00c0 vous de jouer:\n",
      "\n",
      "1. Exprimez la fonction objectif pour un seul exemple\n",
      "2. Exprimez les d\u00e9riv\u00e9es partielles $\\frac{\\partial J^{(i)}}{\\partial b} et \\frac{\\partial J^{(i)}}{\\partial \\pmb{w}}$ \n",
      "\n",
      "Aide: s\u00e9parer les cas $(\\pmb{w}^T x^{(i)} + b)t^{(i)} < 0$ et $(\\pmb{w}^T x^{(i)} + b)t^{(i)} \\geq 0$\n",
      "\n",
      "### Algorithme d'entrainement\n",
      "\n",
      "L'algorithme d'apprentissage est donc:\n",
      "\n",
      " - boucler sur les exemples un par un\n",
      " - pour chaque exemple mettre \u00e0 jour les poids en utilisant : $b \\leftarrow b - \\eta \\frac{\\partial J^{(i)}}{\\partial b}$ et $\\pmb{w} \\leftarrow \\pmb{w} - \\eta \\frac{\\partial J^{(i)}}{\\partial \\pmb{w}}$\n",
      "\n",
      "On peut ajouter une condition d'arr\u00eat :\n",
      "\n",
      " - si tous les exemples d'entrainement sont bien class\u00e9s on s'arr\u00eate. Dans ce cas $J_{perceptron}(\\theta) = 0$ et les d\u00e9riv\u00e9es partielles sont nulles, donc on n'apprend plus rien.\n",
      "\n",
      "## Travail Pratique\n",
      "\n",
      "*Dans cette partie vous allez impl\u00e9menter les formules obtenues ci-dessus*\n",
      "\n",
      "### Pr\u00e9paration des donn\u00e9es\n",
      "\n",
      "Dans ce travail pratique on va travailler sur le dataset iris. On va utiliser seulement 2 classes: les iris avec les \u00e9tiquettes 1 et 2, que l'on va transformer en 1 et -1 pour les besoins du perceptron. On va \u00e9galement utiliser uniquement 2 traits par iris, afin de pouvoir visualiser l'algorithme.\n",
      "\n",
      "Voici le code qui pr\u00e9pare les donn\u00e9es. N'h\u00e9sitez pas \u00e0 regarder les *shapes* des diff\u00e9rents sets pour voir comment les donn\u00e9es sont pr\u00e9par\u00e9es!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import numpy as np\n",
      "\n",
      "#On commence par charger iris\n",
      "iris = np.loadtxt('iris.txt')\n",
      "data = iris\n",
      "\n",
      "# On se limite au cas de la classification BINAIRE donc on va seulement garder \n",
      "# donn\u00e9es des 2 premi\u00e8res classes.\n",
      "# Ici on garde juste les exemples avec l'etiquette 1 et 2.\n",
      "data = data[data[:,-1]<3,:]\n",
      "# Ici on transforme chaque etiquette qui est egale a 2 en -1, pour avoir les \n",
      "# m\u00eames \u00e9tiquettes que dans la formulation standard du perceptron (1 et -1).\n",
      "data[data[:,-1]==2,-1] = -1\n",
      "\n",
      "# On se limite \u00e0 des donn\u00e9es dont la dimension est 2, de fa\u00e7on \u00e0 pouvoir visualiser\n",
      "# la fronti\u00e8re de decision avec la fonction gridplot.\n",
      "train_cols = [2,3]\n",
      "# Une variable pour contenir l'indice de la colonne correspondant aux \u00e9tiquettes.\n",
      "target_ind = [data.shape[1] - 1]\n",
      "\n",
      "# Nombre de classes\n",
      "n_classes = 2\n",
      "# Nombre de points d'entrainement\n",
      "n_train = 75\n",
      "\n",
      "# Commenter pour avoir des resultats non-deterministes \n",
      "np.random.seed(2)\n",
      "\n",
      "# D\u00e9terminer au hasard des indices pour les exemples d'entrainement et de test\n",
      "inds = range(data.shape[0])\n",
      "np.random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "    \n",
      "# S\u00e9parer les donnees dans les deux ensembles: entrainement et test.\n",
      "train_set = data[train_inds,:]  # garder les bonnes lignes\n",
      "train_set = train_set[:,train_cols + target_ind]  # garder les bonnes colonnes\n",
      "test_set = data[test_inds,:]\n",
      "test_set = test_set[:,train_cols + target_ind]\n",
      "\n",
      "# S\u00e9pararer l'ensemble de test: entr\u00e9es et \u00e9tiquettes.\n",
      "test_inputs = test_set[:,:-1]\n",
      "test_labels = test_set[:,-1]\n",
      "\n",
      "# Le taux d'apprentissage\n",
      "eta = 0.1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### La classe Perceptron\n",
      "\n",
      "On introduit la classe *Perceptron*. Comme d'habitude, c'est un algorithme qui poss\u00e8de une fonction *train* pour entra\u00eener l'algorithme \u00e0 partir du *train_set* et une fonction *compute_prediction*, qui pr\u00e9dit les classes de chaque exemple de *test_inputs*.\n",
      "\n",
      "Pour ce travail pratique vous devez:\n",
      "\n",
      "1. Compl\u00e9ter la fonction *train* de la classe *Perceptron*.\n",
      "2. Compl\u00e9ter la fonction *compute_prediction* de la classe *Perceptron*.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Perceptron:\n",
      "\n",
      "    def __init__(self, eta):\n",
      "        \"\"\"\n",
      "        Constructeur de la classe. Prend les param\u00e8tres donn\u00e9es \u00e0 la\n",
      "        constuction de la classe et initialise ses attribues.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        eta : float\n",
      "            Taux d'apprentissage\n",
      "        \"\"\"\n",
      "        self.eta = eta\n",
      "\n",
      "    def plot_function(self, train_data, title):\n",
      "        plt.figure()\n",
      "        d1 = train_data[train_data[:, -1] > 0]\n",
      "        d2 = train_data[train_data[:, -1] < 0]\n",
      "        plt.scatter(d1[:, 0], d1[:, 1], c='b', label='classe +1')\n",
      "        plt.scatter(d2[:, 0], d2[:, 1], c='g', label='classe -1')\n",
      "        x = np.linspace(-10, 10, 100)\n",
      "        y = -(self.weights[0]*x + self.bias)/self.weights[1]\n",
      "        plt.plot(x, y, c='r', lw=2, label='y = -(w1*x + b1)/w2')\n",
      "        plt.xlim(np.min(train_data[:, 0]) - 0.5, np.max(train_data[:, 0]) + 0.5)\n",
      "        plt.ylim(np.min(train_data[:, 1]) - 0.5, np.max(train_data[:, 1]) + 0.5)\n",
      "        plt.grid()\n",
      "        plt.legend(loc='lower right')\n",
      "        plt.title(title)\n",
      "        plt.show()\n",
      "\n",
      "    def train(self, train_data, max_iter=10):\n",
      "        \"\"\"\n",
      "        Entraine le mod\u00e8le d'apprentissage \u00e0 partir d'une matrice de donn\u00e9es,\n",
      "        en faisant un maximum de max_iter sur le set d'entra\u00eenement.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        train_data : array\n",
      "            matrice de dimension (n,d+1) o\u00f9 n est le nombre d'exemples et \n",
      "            d le nombre de dimensions. L'index d+1 de change ligne repr\u00e9sente\n",
      "            la classe de l'exemple de cette ligne.\n",
      "        max_iter : int\n",
      "            Nombre maximum d'it\u00e9rations sur le set d'entrainement (d\u00e9faut 10).\n",
      "        \"\"\"\n",
      "        # 1) Initialisation des param\u00e8tres. \n",
      "        # Initialisez les poids \u00e0\u00a0de petites valeurs et le biais \u00e0 0.\n",
      "        self.weights = #compl\u00e9ter\n",
      "        self.bias = #compl\u00e9ter\n",
      "         \n",
      "        # 2) Entrainement\n",
      "        # Impl\u00e9mentez l'algorithme pr\u00e9sent\u00e9 ci-dessus.\n",
      "        # iterations est le nombre d'it\u00e9rations effectu\u00e9es sur le set d'entra\u00eenement\n",
      "        # count est le nombre d'\u00e9l\u00e9ments mal class\u00e9s.\n",
      "        print 'Entra\u00eenement ...'\n",
      "        iteration = 0\n",
      "        while iteration < max_iter:\n",
      "            self.plot_function(train_data, 'Iteration no: ' + str(iteration))\n",
      "            count = 0\n",
      "            # impl\u00e9mentez l'algorithme d'entrainement ici!\n",
      "            \n",
      "            iteration += 1\n",
      "        print 'Entra\u00eenement termin\u00e9!'\n",
      "        print \"L'erreur d'entra\u00eenement est de \", float(count)/train_data.shape[0], \"%.\"\n",
      "\n",
      "    def compute_predictions(self, test_data):\n",
      "        \"\"\"\n",
      "        Calcule les pr\u00e9dictions \u00e0 partir d'une matrice de test. Donne en\n",
      "        sortie une pr\u00e9dictions pour chaque classe.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        test_data : array\n",
      "            matrice de dimension (n,d) o\u00f9 n est le nombre d'exemples et\n",
      "            d le nombre de dimensions\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        array\n",
      "            tableau de dimension (n) o\u00f9 n est le nombre d'exemples de test.\n",
      "        \"\"\"\n",
      "        \n",
      "        # A COMPL\u00c9TER!\n",
      "        # Cette fonction doit utiliser les param\u00e8tres appris pour calculer\n",
      "        # la valeur de sortie pour les exemples de test_data (un exemple\n",
      "        # par ligne, seulement les traits).\n",
      "  \n",
      "        # 1) Vous devez calculer la vraie valeur de sorties.\n",
      "        predictions = np.zeros(test_data.shape[0])\n",
      "        # compl\u00e9ter ici\n",
      "        \n",
      "        return predictions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Entra\u00eenement du mod\u00e8le\n",
      "\n",
      "Maintenant que la classe *Perceptron* est compl\u00e9t\u00e9e, on peut l'entra\u00eener. Un graphe va s'afficher pour chaque it\u00e9ration effectu\u00e9es sur le set d'entra\u00eenement."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Cr\u00e9er et entrainer le modele\n",
      "model_perceptron = Perceptron(eta)\n",
      "model_perceptron.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Test du mod\u00e8le\n",
      "\n",
      "Maintenant que le mod\u00e8le est entra\u00een\u00e9, on peut le tester!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Obtenir les classes pr\u00e9dites sur l'ensemble de test\n",
      "predictions = model_perceptron.compute_predictions(test_inputs)\n",
      "\n",
      "# Convertir les sorties en classe. On prend le signe.\n",
      "classes_pred = np.sign(predictions)\n",
      "   \n",
      "# Mesurer la performance.\n",
      "err = 1.0 - np.mean(test_labels==classes_pred)\n",
      "\n",
      "model_perceptron.plot_function(test_set, 'Test data')\n",
      "print \"L'eureur de test est de \", 100.0 * err,\"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Si vous avez termin\u00e9...\n",
      "\n",
      "Si vous avez termin\u00e9, vous pouvez essayer de varier les diff\u00e9rents param\u00e8tres, par exemple:\n",
      "\n",
      "1. Varier $\\eta$. Quel est son impact sur le temps d'entra\u00eenement? Et sur les performances?\n",
      "2. Utiliser d'autres traits des iris (par exemple [1,3] \u00e0 la place de [2,3]).\n",
      "3. Varier la taille du set d'entra\u00eenement. Y a-t-il un impact sur les performances?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}