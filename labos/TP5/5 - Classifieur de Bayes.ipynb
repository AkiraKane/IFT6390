{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Démonstration 5: Classifieur de Bayes. Maximum de Vraisemblance pour une densité Gaussienne multivariée. 03/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commencez, si nécessaire, par vous rappeler les notes du cours sur le [classifieur de Bayes](https://studium.umontreal.ca/pluginfile.php/2636322/mod_resource/content/3/7_classifieur_bayes.pdf) et le principe de [maximum de vraisemblance](https://studium.umontreal.ca/pluginfile.php/2636388/mod_resource/content/1/maxvraisemblancegaussienne.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description de haut niveau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aujourd'hui on va construire un **classifieur multiclasse de Bayes**. Ça veut dire qu'au lieu de modéliser $p(\\mbox{classe}|\\mbox{exemple})$ (ou $p(y|x)$), on va plutôt utiliser l'équation de Bayes \n",
    "\n",
    "$$p(\\mbox{classe}|\\mbox{exemple}) = \\frac{p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe})}{\\sum_{c'=1}^{m}p_\\mbox{c'}(x)P_\\mbox{c'}}$$\n",
    "\n",
    "et modéliser les différents morceaux. En fait, on a juste besoin de modéliser le numérateur puisque le dénominateur est une constante de normalisation. De plus, $ P_\\mbox{c'} = n_c / n $\n",
    "\n",
    "Le terme $p(\\mbox{classe})$ représente la probabilité à priori d'une classe, c'est-à-dire notre croyance à priori - avant d'avoir vu un exemple en particulier - sur la probabilité qu'un exemple inconnu appartienne à cette classe). On va représenter cette croyance à priori pour une classe par la fréquence de cette dernière dans les données d'entraînement: $\\frac{n_c}{n}$ où $n_c$ = nombre d'exemple de la classe $c$, puis $n$ = nombre d'exemple d'entraînement. \n",
    "\n",
    "On va utiliser des **densités Gaussiennes multivariées** pour modéliser les différents $p(\\mbox{exemple}|\\mbox{classe})$. Cela veut dire que pour chaque classe, on va supposer que la \"vrai\" distribution $p(\\mbox{exemple}|\\mbox{classe})$ possède la forme d'une Gaussienne multivariée dont on va tenter d'apprendre les paramètres $\\mu$ et $\\Sigma$. En pratique, on va se limiter aujourd'hui à un cas particulier de cette distribution: celui où l'on suppose que la matrice de covariance $\\Sigma$ de chaque Gaussienne est diagonale et que chaque élément de cette diagonale est le même, soit sigma_sq ( <=> \"sigma square\" <=> $\\sigma^2$ <=> la variance). On possède donc un seul paramètre pour contrôler la forme de la covariance. C'est plus simple (pour nous et pour l'ordinateur) à calculer, mais ça signifie aussi que notre modèle est moins puissant. \n",
    "\n",
    "On a donc un modèle paramétrique très simple. Les paramètres sont la moyenne $\\mu$ (un vecteur de dimension celle de l'entrée du système) et la variance $\\sigma^2$ (un seul scalaire dans notre modèle simple, qui va multiplier la matrice identité). L'apprentissage dans ce modèle se fera aujourd'hui par l'application du **principe de maximum de vraisemblance**. Pour chaque classe, on va trouver les valeurs des paramètres qui maximisent la log-vraisemblance des données d'entraînement issus de cette classe: \n",
    "\n",
    "$$\\log\\prod_i^n p(X=x_i)$$\n",
    "\n",
    "Voici le [détail du calcul](https://studium.umontreal.ca/mod/resource/view.php?id=416973) par maximum de vraisemblance de $\\mu$ et $\\sigma^2$ dans le cas qui nous intéresse, soit une gaussienne isotropique.\n",
    "\n",
    "Ayant trouvé les paramètres qui maximisent la vraisemblance pour chacune des classes, il nous est possible de calculer tous les $p(\\mbox{exemple}|\\mbox{classe})$. Il suffit maintenant d'appliquer la règle de Bayes afin de pouvoir classifier un nouvel exemple. Plus précisément, on voudra choisir, pour un exemple, la classe qui maximise $p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe})$ ou encore $log(p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe}))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code à compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargez le fichier [utilitaires.py](https://studium.umontreal.ca/mod/resource/view.php?id=459041) dans le dossier où se trouve vos fichier notebook. Il contient les fonctions utiles que vous avez vus au dernier cours. Vous pourez ainsi les utiliser sans qu'elles encombre votre notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pour la classe `gauss_mv`:\n",
    " \n",
    " - calculez sigma_sq ($\\sigma^2$), la variance dans `gauss_mv.train`\n",
    " - calculez la valeur de la fonction de densité Gaussienne dans `gauss_mv.compute_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "import numpy as np\n",
    "import utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class gauss_mv:\n",
    "    def __init__(self,n_dims,cov_type=\"isotropique\"):\n",
    "        self.n_dims = n_dims\n",
    "        self.mu = np.zeros((1,n_dims))\n",
    "        self.cov_type = cov_type\n",
    "        \n",
    "        if cov_type==\"isotropique\":\n",
    "            # on peut sauver uniquement la diagonale de la matrice de covariance car tous les elements hors-diagonale sont nuls dans \n",
    "            # le cas isotropique\n",
    "            self.sigma_sq = 1.0\n",
    "        if cov_type==\"full\":\n",
    "            pass\n",
    "\n",
    "\t# Pour un ensemble d'entrainement, la fonction devrait calculer l'estimateur par MV de la moyenne et de la matrice de covariance\n",
    "    def train(self, train_data):\n",
    "        self.mu = np.mean(train_data,axis=0)\n",
    "        if self.cov_type == \"isotropique\":\n",
    "            # ici il faut trouver la variance des donnees train_data et la\n",
    "            # mettre dans self.sigma_sq\n",
    "            \n",
    "            self.sigma_sq = np.sum((train_data - self.mu)**2.0) / (train_data.shape[0]*self.n_dims)\n",
    "        if self.cov_type == \"full\":\n",
    "            pass\n",
    "\n",
    "\t# Retourne un vecteur de taille nb. ex. de test contenant les log\n",
    "\t# probabilités de chaque exemple de test sous le modèle.\t\n",
    "    def compute_predictions(self, test_data):\n",
    "        # decommentez la ligne suivante une fois que vous avez complete le\n",
    "        # calcul du log_prob\n",
    "#         log_prob = -np.ones((test_data.shape[0],1))\n",
    "\n",
    "        if self.cov_type == \"isotropique\":\n",
    "            # la ligne suivante calcule log(constante de normalisation)\n",
    "            c = -self.n_dims * np.log(2*np.pi)/2 - self.n_dims*np.log(np.sqrt(self.sigma_sq))            \n",
    "            # il faut calculer la valeur de la log-probabilite de chaque exemple\n",
    "            # de test sous le modele determine par mu et sigma_sq. le vecteur\n",
    "            # des probabilites est/sera log_prob\n",
    "            \n",
    "            log_prob = c - np.sum((test_data - self.mu)**2.0, axis=1)/(2*self.sigma_sq)\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la classe `classif_bayes`:\n",
    "\n",
    " - complétez `classif_bayes.compute_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class classif_bayes:\n",
    "\n",
    "    def __init__(self,modeles_mv, priors):\n",
    "        self.modeles_mv = modeles_mv\n",
    "        self.priors = priors\n",
    "        if len(self.modeles_mv) != len(self.priors):\n",
    "            print 'Le nombre de modeles MV doit etre egale au nombre de priors!'\n",
    "        self.n_classes = len(self.modeles_mv)\n",
    "\t\t\t\n",
    "    # Retourne une matrice de taille nb. ex. de test x nombre de classes contenant les log\n",
    "    # probabilités de chaque exemple de test sous chaque modèle, entrainé par MV.\t\n",
    "    def compute_predictions(self, test_data):\n",
    "        log_pred = np.empty((test_data.shape[0],self.n_classes))\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            # ici il va falloir utiliser modeles_mv[i] et priors pour remplir\n",
    "            # chaque colonne de log_pred (c'est plus efficace de faire tout une\n",
    "            # colonne a la fois)\n",
    "\n",
    "            # log_pred[:,i] =\n",
    "            log_pred[:,i] = self.modeles_mv[i].compute_predictions(test_data) + self.priors[i]\n",
    "\n",
    "        return log_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminez le code en calculant le maximum par classe et en affichant le graphique de la surface de décision à l'aide des fonctions dans [utilitaires.py](https://studium.umontreal.ca/mod/resource/view.php?id=459041)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_cols' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3439c7c0f677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#on peut maintenant calculer les logs-probabilites selon nos modeles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mlog_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifieur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mclassesPred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_cols' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "train_cols = [0,1]\n",
    "target_cols = [-1]\n",
    "\n",
    "iris=np.loadtxt('../iris.txt')\n",
    "iris_train1=iris[0:50,:-1]\n",
    "iris_train2=iris[50:100,:-1]\n",
    "iris_train3=iris[100:150,:-1]\n",
    "\n",
    "# On cree un modele par classe (par maximum de vraissemblance)\n",
    "model_classe1=gauss_mv(4)\n",
    "model_classe2=gauss_mv(4)\n",
    "model_classe3=gauss_mv(4)\n",
    "model_classe1.train(iris_train1)\n",
    "model_classe2.train(iris_train2)\n",
    "model_classe3.train(iris_train3)\n",
    "\n",
    "# On cree une liste de tous nos modeles\n",
    "# On fait la meme chose pour les priors\n",
    "# Les priors sont calcules ici de facon exact car on connait le nombre \n",
    "# de representants par classes. Un fois que vous aurez cree un\n",
    "# ensemble de train/test, il va faloir les calculer de facon exacte\n",
    "modele_mv=[model_classe1,model_classe2,model_classe3]\n",
    "priors=[0.3333,0.3333,0.3333]\n",
    "\n",
    "# On cree notre classifieur avec notre liste de modeles gaussien et nos priors\n",
    "classifieur=classif_bayes(modele_mv,priors)\n",
    "\n",
    "#on peut maintenant calculer les logs-probabilites selon nos modeles\n",
    "log_prob=classifieur.compute_predictions(iris[:,train_cols])\n",
    "\n",
    "classesPred = log_prob.argmax(1)+1\n",
    "\n",
    "utilitaires.teste(classesPred, iris[:,-1])\n",
    "print \"Taux d'erreur %.2f%%\" % ((1-(classesPred==iris[:,-1]).mean())*100.0)\n",
    "\n",
    "# il reste maintenant a calculer le maximum par classe pour la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Si vous avez terminé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Modifiez votre code pour que `gauss_mv` calcule une matrice de covariance diagonale (où l'on estime la variance pour chaque composante/trait de l'entrée) ou même une matrice de covariance pleine.\n",
    " - Les commandes `numpy.cov` et `numpy.var` vous seront probablement utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}